import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from peft import PeftModel
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from transformers import TextIteratorStreamer
from threading import Thread

# --- YapÄ±landÄ±rma ---
CURRENT_DIR = os.getcwd()
PERSIST_DIRECTORY = os.path.join(CURRENT_DIR, "chroma_db")
EMBEDDING_MODEL_NAME = "emrecan/bert-base-turkish-cased-mean-nli-stsb-tr"
# Adapter config'den aldÄ±ÄŸÄ±mÄ±z temel model
BASE_MODEL_NAME = "google/gemma-3-4b-it"
ADAPTER_PATH = os.path.join(CURRENT_DIR, "fine_tuned_models", "gemma3-4b-lora-final")

# Modeli global olarak yÃ¼kleyelim ki her istekte tekrar yÃ¼klenmesin (API iÃ§in)
_global_model = None
_global_tokenizer = None

def load_retriever(k: int = 3):
    """
    ChromaDB vektÃ¶r veritabanÄ±nÄ± yÃ¼kler ve bir retriever dÃ¶ndÃ¼rÃ¼r.
    k: Getirilecek en alakalÄ± belge sayÄ±sÄ±.
    """
    print(f"[INFO] Veritabanina baglaniliyor: {PERSIST_DIRECTORY}")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    vectorstore = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=embeddings,
        collection_name="enerji_mevzuati"
    )
    
    return vectorstore.as_retriever(search_kwargs={"k": k})

def load_llm_streaming():
    """
    Streaming destekli model ve tokenizer'Ä± yÃ¼kler.
    Geriye (model, tokenizer) dÃ¶ner.
    """
    global _global_model, _global_tokenizer
    
    if _global_model is not None:
        return _global_model, _global_tokenizer

    print(f"[INFO] Tokenizer yukleniyor: {ADAPTER_PATH}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)
    except:
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)

    print(f"[INFO] Temel Model Yukleniyor: {BASE_MODEL_NAME}")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    print(f"[INFO] LoRA Adapter Entegre Ediliyor: {ADAPTER_PATH}")
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    model = model.merge_and_unload()
    
    _global_model = model
    _global_tokenizer = tokenizer
    
    return model, tokenizer

def format_docs(docs):
    # Belgeler arasÄ±na net bir ayraÃ§ koyarak modelin karÄ±ÅŸmamasÄ±nÄ± saÄŸlayalÄ±m
    return "\n\n--- YENÄ° BELGE ---\n\n".join(doc.page_content for doc in docs)

def get_rag_chain_streaming(question: str, context: str):
    """
    Generator fonksiyonu: Verilen baÄŸlam ve soruya gÃ¶re cevabÄ± parÃ§a parÃ§a Ã¼retir.
    """
    model, tokenizer = load_llm_streaming()
    
    # Not: Retrieval iÅŸlemi dÄ±ÅŸarÄ±da (API katmanÄ±nda) yapÄ±lÄ±p buraya sadece metin (context) gelecek.
    
    # 2. Prompt HazÄ±rla
    template = """<start_of_turn>user
Sen TÃ¼rkiye Enerji MevzuatÄ± konusunda uzman, yardÄ±msever bir asistansÄ±n. 
AÅŸaÄŸÄ±da farklÄ± kaynaklardan alÄ±nmÄ±ÅŸ 'Mevzuat BaÄŸlamÄ±' parÃ§alarÄ± verilmiÅŸtir.
Bu parÃ§alardaki bilgileri BÄ°RLEÅTÄ°REREK ve SENTEZLEYEREK soruyu detaylÄ±ca cevapla.
EÄŸer bir cÃ¼mle yarÄ±m kalmÄ±ÅŸsa (kesilmiÅŸse), o cÃ¼mleyi ihmal et.
Sadece verilen baÄŸlamdaki bilgileri kullan, dÄ±ÅŸarÄ±dan bilgi ekleme.
EÄŸer cevap baÄŸlamda hiÃ§ yoksa "Verilen mevzuat metinlerinde bu sorunun cevabÄ± bulunmamaktadÄ±r." de.

Mevzuat BaÄŸlamÄ±:
{context}

Soru:
{question}<end_of_turn>
<start_of_turn>model
"""
    prompt_text = template.format(context=context, question=question)
    
    # --- DETAYLI LOGLAMA ---
    print("\n" + "="*50)
    print(f"[PROMPT] LLM'e Giden Metin ({len(prompt_text)} karakter):")
    print("-" * 20)
    print(prompt_text.replace(context, f"[...BAÄLAM ({len(context)} karakter)...]")) # BaÄŸlamÄ± kÄ±saltarak gÃ¶ster
    print("="*50 + "\n")
    # -----------------------

    inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)
    
    # 3. Streamer OluÅŸtur
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    # 4. Ãœretimi AyrÄ± Thread'de BaÅŸlat
    generation_kwargs = dict(
        **inputs, 
        streamer=streamer, 
        max_new_tokens=1024,      # Daha uzun cevaplar iÃ§in artÄ±rÄ±ldÄ±
        temperature=0.3,          # Biraz daha akÄ±cÄ±lÄ±k iÃ§in artÄ±rÄ±ldÄ±
        do_sample=True,
        top_p=0.95,
        repetition_penalty=1.05   # Ã‡ok katÄ± olmamasÄ± iÃ§in dÃ¼ÅŸÃ¼rÃ¼ldÃ¼
    )
    
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    
    # 5. Token'larÄ± Yakala ve GÃ¶nder
    for new_text in streamer:
        yield new_text


if __name__ == "__main__":
    # Dosya doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rsa test modu baÅŸlar
    print("--- Enerji MevzuatÄ± Chatbot (Test Modu) ---")
    chain = get_rag_chain()
    
    while True:
        try:
            query = input("\nâ“ Soru (Ã‡Ä±kÄ±ÅŸ iÃ§in 'q'): ")
            if query.lower() == 'q':
                break
            
            print("â³ DÃ¼ÅŸÃ¼nÃ¼yor...")
            result = chain.invoke({"query": query})
            
            print("\nğŸ¤– Cevap:")
            print(result['result'].strip())
            
            print("\nğŸ“š Kaynaklar:")
            seen_sources = set()
            for doc in result['source_documents']:
                source = doc.metadata.get('source_file', 'Bilinmiyor')
                article = doc.metadata.get('article_number', 'Belirsiz')
                key = f"{source} - {article}"
                if key not in seen_sources:
                    print(f"- {key}")
                    seen_sources.add(key)
                    
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"âŒ Hata oluÅŸtu: {e}")
